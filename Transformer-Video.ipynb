{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnSTUaHlt-UP"
   },
   "source": [
    "# Import things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ozZz5aQVRnXR",
    "outputId": "1a6c9f46-ff26-4aeb-c779-35ce156b551e"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OLF7P_IAaLTq",
    "outputId": "6b029205-36df-4da7-e321-222f2dad3cf8"
   },
   "outputs": [],
   "source": [
    "!pip install pytorchvideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SnP4EgV9rwkr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inskZiZJemdS"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Twn2JMT1tbio"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "ROOT_DIR = \"combined_data_v3\"\n",
    "LOG_INTERVAL = 10\n",
    "NUM_VEHICLE = 6\n",
    "config_path = None\n",
    "if config_path:\n",
    "  assert os.path.exists(config_path), \"Config file not found\"\n",
    "  with open(config_path, \"r\") as f:\n",
    "    config = json.loads(f.read())\n",
    "else:\n",
    "  config = {\n",
    "      'video_encoder': {\n",
    "          'size': 'XS'\n",
    "      },\n",
    "      'path_encoder': {\n",
    "          'dim_feedforward': 2048,\n",
    "          'n_hidden': 128,\n",
    "          'n_head': 4,\n",
    "          'n_layers': 4,\n",
    "          'dropout': 0.2,\n",
    "          'out_dim': 256,\n",
    "      },\n",
    "      'cross_net': {\n",
    "          'n_hidden': 512,\n",
    "          'dropout': 0.2,\n",
    "      },\n",
    "      'batch_size': 4,\n",
    "      'lr1': 1e-4,\n",
    "      'lr2': 3e-6,\n",
    "      'n_epoch_1': 0,\n",
    "      'n_epoch_2': 20,\n",
    "      'seq_len': 256,\n",
    "      'n_frames': 8,\n",
    "      'img_size': 256,\n",
    "      'mean': [0.45, 0.45, 0.45],\n",
    "      'std': [0.225, 0.225, 0.225],\n",
    "      'n_samples': [631, 700],\n",
    "      'aug': 0.15\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqU4FT-Urk9c"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z49eZNzbvKJq"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDceQWLfOXaP"
   },
   "source": [
    "## Path Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLM14KvFxUQm"
   },
   "outputs": [],
   "source": [
    "class PathEncoder(nn.Module):\n",
    "  def __init__(self, conf, in_channels=4):\n",
    "    super(PathEncoder, self).__init__()\n",
    "    self.fc1 = nn.Linear(in_channels, conf[\"n_hidden\"])\n",
    "    encoder_layer = nn.TransformerEncoderLayer(d_model=conf[\"n_hidden\"], nhead=conf[\"n_head\"], dim_feedforward=conf['dim_feedforward'])\n",
    "    self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=conf[\"n_layers\"])\n",
    "    self.fc2 = nn.Linear(conf[\"n_hidden\"]*config['seq_len'], conf[\"out_dim\"])\n",
    "    self.drop2 = nn.Dropout(p=conf[\"dropout\"])\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = self.transformer_encoder(x)\n",
    "    x = torch.flatten(x, start_dim=1, end_dim=-1)\n",
    "    x = self.drop2(x)\n",
    "    x = F.relu(self.fc2(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfCZK9M2Oar7"
   },
   "source": [
    "## Video Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQ5VV9bzy3Nz"
   },
   "outputs": [],
   "source": [
    "model_transform_params  = {\n",
    "    \"x3d_xs\": {\n",
    "        \"side_size\": 182,\n",
    "        \"crop_size\": 182,\n",
    "        \"num_frames\": 4,\n",
    "        \"sampling_rate\": 12,\n",
    "    },\n",
    "    \"x3d_s\": {\n",
    "        \"side_size\": 182,\n",
    "        \"crop_size\": 182,\n",
    "        \"num_frames\": 13,\n",
    "        \"sampling_rate\": 6,\n",
    "    },\n",
    "    \"x3d_m\": {\n",
    "        \"side_size\": 256,\n",
    "        \"crop_size\": 256,\n",
    "        \"num_frames\": 16,\n",
    "        \"sampling_rate\": 5,\n",
    "    }\n",
    "}\n",
    "transform_params = model_transform_params[ 'x3d_{}'.format(config['video_encoder']['size'].lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmTMLXowneqE"
   },
   "outputs": [],
   "source": [
    "from pytorchvideo.models.accelerator.mobile_cpu.efficient_x3d import EfficientX3d\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "class VideoEncoder(nn.Module):\n",
    "  def __init__(self, conf):\n",
    "    super(VideoEncoder, self).__init__()\n",
    "    self.model = EfficientX3d(expansion=conf['size'], head_act='identity')\n",
    "    \n",
    "    checkpoint_path = \"https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/efficient_x3d_{}_original_form.pyth\".format(conf['size'].lower())\n",
    "    checkpoint = load_state_dict_from_url(checkpoint_path)\n",
    "    self.model.load_state_dict(checkpoint)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ot-aNAQOjUJ"
   },
   "source": [
    "## CrossNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixi2omVsxlCu"
   },
   "outputs": [],
   "source": [
    "class CrossNet(nn.Module):\n",
    "  def __init__(self, conf):\n",
    "    super(CrossNet, self).__init__()\n",
    "    self.path_encoder = PathEncoder(conf['path_encoder'], in_channels=4)\n",
    "    self.video_encoder = VideoEncoder(conf['video_encoder'])\n",
    "    num_features = 400 + conf['path_encoder']['out_dim']\n",
    "    self.fc1 = nn.Linear(num_features, conf['cross_net']['n_hidden'])\n",
    "    self.fc2 = nn.Linear(conf['cross_net']['n_hidden'], 2)\n",
    "    self.drop = nn.Dropout(p=conf['cross_net'][\"dropout\"])\n",
    "  \n",
    "  def forward(self, video, frames):\n",
    "    video_encoding = self.video_encoder(video) # B * 400\n",
    "    path_encoding = self.path_encoder(frames) # B * conf['path_encoder']['out_dim']\n",
    "    \n",
    "    encodings = torch.cat((video_encoding, path_encoding), dim=1)\n",
    "    out = self.drop(F.relu(self.fc1(encodings)))\n",
    "    out = self.fc2(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnRTBYZCre4x"
   },
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QFiGurB-0qfK"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(ROOT_DIR):\n",
    "  !unzip \"/content/drive/MyDrive/Viettel DTalent/mini_project_redlight_running/combined_data_v2.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRsQFzhXrx6r"
   },
   "outputs": [],
   "source": [
    "files = sorted(os.listdir(os.path.join(ROOT_DIR, \"processed_labels\")))\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmFtdUDyruhx"
   },
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQB78w6bouQm"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TRAIN_SIZE = 0.6\n",
    "VAL_SIZE = 0.2\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "assert TRAIN_SIZE + VAL_SIZE + TEST_SIZE == 1\n",
    "\n",
    "train_files, test_files = train_test_split(files, test_size=0.2, random_state=12)\n",
    "train_files, val_files = train_test_split(train_files, test_size=VAL_SIZE/(1-TEST_SIZE), random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWfTX28LriZc"
   },
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GbCBtHXbG99"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "\n",
    "class RedLightDataset(Dataset):\n",
    "  def __init__(self, root_dir, label_dir, data, vid_transform=None):\n",
    "    self.label_dir = label_dir\n",
    "    self.root_dir = root_dir\n",
    "    self.data = data\n",
    "    self.vid_transform = vid_transform\n",
    "    self.img_transform = transforms.ToTensor()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def get_data(self, idx):\n",
    "    f = open(os.path.join(self.label_dir, self.data[idx]))\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "  def get_image_from_path(self, img_path):\n",
    "    img = cv2.imread(img_path) # H * W * C\n",
    "    dim = (transform_params[\"crop_size\"], transform_params[\"crop_size\"]) # W, H\n",
    "    img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return self.img_transform(img) # C * H * W\n",
    "\n",
    "  def read_video(self, vid_path):\n",
    "    paths = sorted(os.listdir(vid_path))\n",
    "    video = torch.stack([self.get_image_from_path(os.path.join(vid_path, path)) for path in paths]) # T * C * H * W\n",
    "    video = torch.transpose(video, 0, 1) # C * T * H * W\n",
    "    return video\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    data = self.get_data(idx)\n",
    "\n",
    "    vid_path = os.path.join(self.root_dir, data[\"meta\"][\"vid_path\"])\n",
    "    video = self.read_video(vid_path) # C * T * H * W\n",
    "    frames_bbox = torch.tensor(data[\"frames\"]) # L * 4\n",
    "    label = torch.tensor(int(data[\"meta\"][\"cross\"]))\n",
    "\n",
    "    if self.vid_transform:\n",
    "        video = self.vid_transform(video)\n",
    "\n",
    "    sample={\n",
    "        \"video\": video,\n",
    "        \"frames_bbox\": frames_bbox,\n",
    "        # \"path\": os.path.join(self.label_dir, self.data[idx]),\n",
    "        \"label\": label\n",
    "        }\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yagozjbMfIk3"
   },
   "outputs": [],
   "source": [
    "def pad_sequence_fixed_size(sequences, batch_first=False, padding_value=0.0, max_len=256):\n",
    "  # based on torch.nn.utils.rnn.pad_sequence\n",
    "    max_size = sequences[0].size()\n",
    "    trailing_dims = max_size[1:]\n",
    "    \n",
    "    if batch_first:\n",
    "        out_dims = (len(sequences), max_len) + trailing_dims\n",
    "    else:\n",
    "        out_dims = (max_len, len(sequences)) + trailing_dims\n",
    "\n",
    "    out_tensor = sequences[0].new_full(out_dims, padding_value)\n",
    "    for i, tensor in enumerate(sequences):\n",
    "        length = tensor.size(0)\n",
    "        # use index notation to prevent duplicate references to the tensor\n",
    "        if batch_first:\n",
    "            out_tensor[i, :length, ...] = tensor\n",
    "        else:\n",
    "            out_tensor[:length, i, ...] = tensor\n",
    "\n",
    "    return out_tensor\n",
    "\n",
    "class Collate:\n",
    "  def __call__(self, batch):\n",
    "    videos = [item[\"video\"].unsqueeze(0) for item in batch]\n",
    "    videos = torch.cat(videos, dim=0) # N * C * T * H * W\n",
    "\n",
    "    frames = [item[\"frames_bbox\"] for item in batch]\n",
    "    frames_pad = pad_sequence_fixed_size(frames, batch_first=True, max_len=config['seq_len']) # N * seq_len * 4\n",
    "    \n",
    "    labels = [item[\"label\"].unsqueeze(0) for item in batch] \n",
    "    labels = torch.cat(labels, dim=0) # N\n",
    "\n",
    "    return videos, frames_pad, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8ASfqC5jAbn"
   },
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jD8CPW6mTPR1"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.transforms import (\n",
    "    UniformTemporalSubsample,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rifz19wTjS2L"
   },
   "outputs": [],
   "source": [
    "class VideoRandomColorJitter(object):\n",
    "  # Random Contrast in range [max(0, 1-value), 1+value]\n",
    "  def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n",
    "    self.brightness = brightness\n",
    "    self.contrast = contrast\n",
    "    self.saturation = saturation\n",
    "    self.hue = hue\n",
    "\n",
    "  def apply_color_jitter(self, img, brightness, contrast, saturation, hue):\n",
    "    img = TF.adjust_brightness(img, brightness)\n",
    "    img = TF.adjust_contrast(img, contrast)\n",
    "    img = TF.adjust_saturation(img, saturation)\n",
    "    img = TF.adjust_hue(img, hue)\n",
    "    return img\n",
    "\n",
    "  def __call__(self, video):\n",
    "    _brightness = np.random.uniform(max(0, 1 - self.brightness), 1 + self.brightness)\n",
    "    _contrast = np.random.uniform(max(0, 1 - self.contrast), 1 + self.contrast)\n",
    "    _saturation = np.random.uniform(max(0, 1 - self.saturation), 1 + self.saturation)\n",
    "    _hue = np.random.uniform(-self.hue, self.hue)\n",
    "    \n",
    "    out = [self.apply_color_jitter(video[:,i,:], _brightness, _contrast, _saturation, _hue).unsqueeze(1) for i in range(video.shape[1])]\n",
    "\n",
    "    return torch.cat(out,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1QLI5pZjQTH"
   },
   "outputs": [],
   "source": [
    "train_vid_transform = Compose([\n",
    "                          UniformTemporalSubsample(transform_params[\"num_frames\"]),\n",
    "                          VideoRandomColorJitter(brightness=config['aug'], contrast=config['aug'], saturation=config['aug'], hue=config['aug']),\n",
    "                          NormalizeVideo(config['mean'], config['std'])\n",
    "                  ])\n",
    "test_vid_transform = Compose([\n",
    "                          UniformTemporalSubsample(transform_params[\"num_frames\"]),\n",
    "                          NormalizeVideo(config['mean'], config['std'])\n",
    "                  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POTazKqMZ8Ot"
   },
   "outputs": [],
   "source": [
    "train_dataset = RedLightDataset(root_dir=ROOT_DIR,\n",
    "                          label_dir=os.path.join(ROOT_DIR, \"processed_labels\"), \n",
    "                          data=train_files,\n",
    "                          vid_transform=train_vid_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, collate_fn=Collate(), num_workers=2)\n",
    "\n",
    "val_dataset = RedLightDataset(root_dir=ROOT_DIR,\n",
    "                          label_dir=os.path.join(ROOT_DIR, \"processed_labels\"), \n",
    "                          data=val_files,\n",
    "                          vid_transform=test_vid_transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=Collate(), num_workers=2)\n",
    "\n",
    "test_dataset = RedLightDataset(root_dir=ROOT_DIR,\n",
    "                          label_dir=os.path.join(ROOT_DIR, \"processed_labels\"), \n",
    "                          data=test_files,\n",
    "                          vid_transform=test_vid_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=Collate(), num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CVlzwO3rjrN"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jia5pbeaJw_"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time \n",
    "import copy\n",
    "\n",
    "def train(model, hist, dataloaders, criterion, optimizer, n_epochs, get_best_model=False):\n",
    "  since = time.time()\n",
    "\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "  \n",
    "  best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "  best_loss = 1e9\n",
    "  best_f1 = 0\n",
    "  best_precision = 0\n",
    "  best_recall = 0\n",
    "\n",
    "  best_train_loss = 1e9\n",
    "  best_train_f1 = 0\n",
    "  best_train_precision = 0\n",
    "  best_train_recall = 0\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, n_epochs - 1))\n",
    "    print('-' * 10) \n",
    "    for phase in ['train', 'val']:\n",
    "      if phase == 'train':\n",
    "        model.train()\n",
    "      else:\n",
    "        model.eval()\n",
    "      \n",
    "      running_loss = 0.0\n",
    "      running_corrects = 0\n",
    "      target_true = 0\n",
    "      predicted_true = 0\n",
    "      correct_true = 0\n",
    "\n",
    "      for batch_idx, (videos, frames, labels) in enumerate(dataloaders[phase]):\n",
    "        videos = videos.to(device)\n",
    "        frames = frames.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(phase=='train'):\n",
    "          outputs = model(videos, frames)\n",
    "          loss = criterion(outputs, labels.squeeze())\n",
    "          # outputs = torch.exp(outputs)\n",
    "          preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "          if phase == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "          \n",
    "          running_loss += loss.item() * config['batch_size']\n",
    "          running_corrects += torch.sum(preds==labels.data)\n",
    "          \n",
    "          # Get data for f1 calculation\n",
    "          target_classes = labels.data\n",
    "          target_true += torch.sum(target_classes == 1).float()\n",
    "          predicted_true += torch.sum(preds).float()\n",
    "          correct_true += torch.sum(target_classes * preds == 1).float()\n",
    "\n",
    "          if phase == 'train' and batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "              epoch, batch_idx * config['batch_size'], len(dataloaders[phase].dataset),\n",
    "              100. * batch_idx / len(dataloaders[phase]), running_loss/((batch_idx+1)*config['batch_size'])))\n",
    "\n",
    "      epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "      epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "      recall = correct_true / target_true\n",
    "      precision = correct_true / predicted_true\n",
    "      f1_score = 2 * precision * recall / (precision + recall)\n",
    "      print('{} Loss: {:.4f} Acc: {:.4f} F1: {:.4f}'.format(phase, epoch_loss, epoch_acc, f1_score))\n",
    "      \n",
    "      if phase == 'val' and best_loss > epoch_loss:\n",
    "        best_f1 = f1_score.item()\n",
    "        best_loss = epoch_loss\n",
    "        best_precision = precision\n",
    "        best_recall = recall\n",
    "\n",
    "        best_train_f1 = hist['train_f1'][-1]\n",
    "        best_train_loss = hist['train_loss'][-1]\n",
    "        best_train_precision =  hist['train_precision'][-1]\n",
    "        best_train_recall =  hist['train_recall'][-1]\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "      if phase == 'val':\n",
    "        hist['val_acc'].append(epoch_acc.item())\n",
    "        hist['val_loss'].append(epoch_loss)\n",
    "        hist['val_f1'].append(f1_score.item())\n",
    "        hist['val_precision'].append(precision.item())\n",
    "        hist['val_recall'].append(recall.item())\n",
    "      else:\n",
    "        hist['train_acc'].append(epoch_acc.item())\n",
    "        hist['train_loss'].append(epoch_loss)\n",
    "        hist['train_f1'].append(f1_score.item())\n",
    "        hist['train_precision'].append(precision.item())\n",
    "        hist['train_recall'].append(recall.item())\n",
    "\n",
    "      print()\n",
    "\n",
    "  time_elapsed = time.time() - since\n",
    "  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "  print('Best val loss: {:.4f} Precision {:.4f} Recall {:.4f} F1: {:.4f}'.format(best_loss, best_precision, best_recall, best_f1))\n",
    "  print('Train loss: {:.4f} Precision {:.4f} Recall {:.4f} F1: {:.4f}'.format(best_train_loss, best_train_precision, best_train_recall, best_train_f1))\n",
    "\n",
    "  if get_best_model:\n",
    "    model.load_state_dict(best_model_wts)\n",
    "  \n",
    "  return model, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItkQk7h1BEC1"
   },
   "outputs": [],
   "source": [
    "def get_class_weight(n_samples):\n",
    "  normedWeights = [1 - (x / sum(n_samples)) for x in n_samples]  \n",
    "  return torch.FloatTensor(normedWeights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "907a6ef0fe6b470e935650e55b6e1f2f",
      "c41a88146614473a8acab313c1217a37",
      "87231d6ea15a43c5a75144a0699c024c",
      "1e83b909c97e43ceab8abd9f62cb2e05",
      "101da796d39a470183ca0ebe41925a6f",
      "135e5770d11941d6b6010a6f29bb6f63",
      "3cf977a263b8499b9672ce8d25a6ea05",
      "dfd4460bda9046618d7d990f51526243"
     ]
    },
    "id": "EUR-eQ-YaJPx",
    "outputId": "922ea1ae-68ca-4818-de15-0c273cf80ac2"
   },
   "outputs": [],
   "source": [
    "dataloaders = {\"train\": train_loader, \"val\": val_loader}\n",
    "weight = get_class_weight(config['n_samples'])\n",
    "criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "# criterion = nn.NLLLoss(weight=weight)\n",
    "\n",
    "model = CrossNet(config)\n",
    "model = model.to(device)\n",
    "\n",
    "hist = {\n",
    "      \"train_loss\": [],\n",
    "      \"val_loss\": [],\n",
    "      \"train_acc\": [],\n",
    "      \"val_acc\": [],\n",
    "      \"train_precision\":[],\n",
    "      \"val_precision\":[],\n",
    "      \"train_recall\":[],\n",
    "      \"val_recall\":[],\n",
    "      \"train_f1\": [],\n",
    "      \"val_f1\": []\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-JbuZHMZghUp",
    "outputId": "7dde1661-0f43-4281-fedc-f22efded596f"
   },
   "outputs": [],
   "source": [
    "# Freeze VideoEncoder\n",
    "trainable_parameters = []\n",
    "for name, p in model.named_parameters():\n",
    "    if \"VideoEncoder\" not in name:\n",
    "        trainable_parameters.append(p)\n",
    "\n",
    "optimizer = optim.Adam(trainable_parameters, lr=config['lr1'])\n",
    "model, hist = train(model, hist, dataloaders, criterion, optimizer, n_epochs=config['n_epoch_1'], get_best_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Th5EPUMNzgOA"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=config['lr2'])\n",
    "model, hist = train(model, hist, dataloaders, criterion, optimizer, n_epochs=config['n_epoch_2'], get_best_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzgNPCacrm6A"
   },
   "source": [
    "# Evaluate and save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RY2UfRxoK1Nd"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def eval(model, dataloader, criterion):\n",
    "  model.eval()\n",
    "\n",
    "  running_loss = 0.0\n",
    "  target_true = 0\n",
    "  predicted_true = 0\n",
    "  correct_true = 0\n",
    "\n",
    "  preds_arr = []\n",
    "  gt_arr = []\n",
    "  for batch_idx, (videos, frames, labels) in enumerate(dataloader):\n",
    "    videos = videos.to(device)\n",
    "    frames = frames.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      outputs = model(videos, frames)\n",
    "      loss = criterion(outputs, labels)\n",
    "      preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "      preds_arr.extend(preds.tolist())\n",
    "      gt_arr.extend(labels.tolist())\n",
    "\n",
    "      running_loss += loss.item() * config['batch_size']\n",
    "      # Get data for f1 calculation\n",
    "      target_classes = labels.data\n",
    "      target_true += torch.sum(target_classes == 1).float()\n",
    "      predicted_true += torch.sum(preds).float()\n",
    "      correct_true += torch.sum(target_classes * preds == 1).float()\n",
    "\n",
    "  epoch_loss = running_loss / len(dataloader.dataset)\n",
    "  recall = correct_true / target_true\n",
    "  precision = correct_true / predicted_true\n",
    "  f1_score = 2 * precision * recall / (precision + recall)\n",
    "  \n",
    "  print()\n",
    "  print(\"Precision: {:.4f}\".format(precision.item()))\n",
    "  print(\"Recall: {:.4f}\".format(recall.item()))\n",
    "  print(\"F1: {:.4f}\".format(f1_score.item()))\n",
    "        \n",
    "  return epoch_loss, f1_score.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GZKHK94ME8Y"
   },
   "outputs": [],
   "source": [
    "hist['test_loss'], hist['test_f1'] = eval(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3q0VBCBFzu0g"
   },
   "outputs": [],
   "source": [
    "print(\"Test loss: {:.4f}\".format(hist['test_loss']))\n",
    "print(\"Test F1: {:.4f}\".format(hist['test_f1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dew9r-eG9N-2"
   },
   "outputs": [],
   "source": [
    "RESULT_PATH = \"model\"\n",
    "ZIP_PATH = \"model.zip\"\n",
    "if not os.path.exists(RESULT_PATH):\n",
    "  os.mkdir(RESULT_PATH)\n",
    "if os.path.exists(ZIP_PATH):\n",
    "  os.remove(ZIP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCS_9_0E9T0_"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfEvXcUQRyhI"
   },
   "outputs": [],
   "source": [
    "val_idx = np.argmin(hist['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1H7EPWPoaLh"
   },
   "outputs": [],
   "source": [
    "plt.title(\"Loss | Best Val loss: {:.4f}\".format(hist['val_loss'][val_idx]))\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.plot(hist['train_loss'],label=\"Train\")\n",
    "plt.plot(hist['val_loss'],label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(RESULT_PATH, \"loss.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71djbmutySBe"
   },
   "outputs": [],
   "source": [
    "plt.title(\"F1 | Best Val F1: {:.4f}\".format(hist['val_f1'][val_idx]))\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "\n",
    "plt.plot(hist['train_f1'],label=\"Train\")\n",
    "plt.plot(hist['val_f1'],label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(RESULT_PATH, \"f1.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_YsGJ695E3a"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(RESULT_PATH, \"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xW_fMAW_KfgA"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(RESULT_PATH, 'config.json'), 'w') as f:\n",
    "  json.dump(config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ME_DaprCMXly"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(RESULT_PATH, 'result.json'), 'w') as f:\n",
    "  json.dump(hist, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "Gn9-bplYL5ml",
    "outputId": "98241b96-829a-4d0e-a38a-6251fd03cef1"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "output_zip = shutil.make_archive(RESULT_PATH, 'zip', RESULT_PATH)\n",
    "files.download(output_zip)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "101da796d39a470183ca0ebe41925a6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "135e5770d11941d6b6010a6f29bb6f63": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e83b909c97e43ceab8abd9f62cb2e05": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dfd4460bda9046618d7d990f51526243",
      "placeholder": "​",
      "style": "IPY_MODEL_3cf977a263b8499b9672ce8d25a6ea05",
      "value": " 14.8M/14.8M [00:00&lt;00:00, 33.9MB/s]"
     }
    },
    "3cf977a263b8499b9672ce8d25a6ea05": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "87231d6ea15a43c5a75144a0699c024c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_135e5770d11941d6b6010a6f29bb6f63",
      "max": 15543903,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_101da796d39a470183ca0ebe41925a6f",
      "value": 15543903
     }
    },
    "907a6ef0fe6b470e935650e55b6e1f2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_87231d6ea15a43c5a75144a0699c024c",
       "IPY_MODEL_1e83b909c97e43ceab8abd9f62cb2e05"
      ],
      "layout": "IPY_MODEL_c41a88146614473a8acab313c1217a37"
     }
    },
    "c41a88146614473a8acab313c1217a37": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dfd4460bda9046618d7d990f51526243": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
